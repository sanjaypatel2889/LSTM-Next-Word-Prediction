# LSTM Next Word Prediction â€“ Shakespeare Text Generation

An end-to-end NLP project that predicts the next word in a sentence using an LSTM neural network trained on Shakespeareâ€™s *Hamlet*. The model is deployed as an interactive web app using Streamlit.

---

## ğŸš€ Features
- Word-level next-word prediction using LSTM
- Trained on classic Shakespearean text
- Interactive UI built with Streamlit
- Robust error handling for unseen words
- Ready for local or cloud deployment

---

## ğŸ§  Model Architecture
- Embedding Layer
- LSTM Layer
- Dense Softmax Output Layer
- Loss: Categorical Crossentropy
- Optimizer: Adam

---

## ğŸ› ï¸ Tech Stack
- Python
- TensorFlow / Keras
- NLP (Tokenization, Padding)
- Streamlit
- Git & GitHub

---

## ğŸ“‚ Project Structure
# LSTM Next Word Prediction â€“ Shakespeare Text Generation

An end-to-end NLP project that predicts the next word in a sentence using an LSTM neural network trained on Shakespeareâ€™s *Hamlet*. The model is deployed as an interactive web app using Streamlit.

---

## ğŸš€ Features
- Word-level next-word prediction using LSTM
- Trained on classic Shakespearean text
- Interactive UI built with Streamlit
- Robust error handling for unseen words
- Ready for local or cloud deployment

---

## ğŸ§  Model Architecture
- Embedding Layer
- LSTM Layer
- Dense Softmax Output Layer
- Loss: Categorical Crossentropy
- Optimizer: Adam

---

## ğŸ› ï¸ Tech Stack
- Python
- TensorFlow / Keras
- NLP (Tokenization, Padding)
- Streamlit
- Git & GitHub

---

## ğŸ“‚ Project Structure
# LSTM Next Word Prediction â€“ Shakespeare Text Generation

An end-to-end NLP project that predicts the next word in a sentence using an LSTM neural network trained on Shakespeareâ€™s *Hamlet*. The model is deployed as an interactive web app using Streamlit.

---

## ğŸš€ Features
- Word-level next-word prediction using LSTM
- Trained on classic Shakespearean text
- Interactive UI built with Streamlit
- Robust error handling for unseen words
- Ready for local or cloud deployment

---

## ğŸ§  Model Architecture
- Embedding Layer
- LSTM Layer
- Dense Softmax Output Layer
- Loss: Categorical Crossentropy
- Optimizer: Adam

---

## ğŸ› ï¸ Tech Stack
- Python
- TensorFlow / Keras
- NLP (Tokenization, Padding)
- Streamlit
- Git & GitHub

---

## ğŸ“‚ Project Structure
â”œâ”€â”€ app.py # Streamlit application
â”œâ”€â”€ experiment.ipynb # Model training notebook
â”œâ”€â”€ next_word_lstm.h5 # Trained LSTM model
â”œâ”€â”€ tokenizer.pickle # Saved tokenizer
â”œâ”€â”€ hamlet.txt # Training corpus
â”œâ”€â”€ requirements.txt # Dependencies
â””â”€â”€ README.md

---

## â–¶ï¸ How to Run Locally

```bash
git clone https://github.com/your-username/LSTM-Next-Word-Prediction.git
cd LSTM-Next-Word-Prediction
pip install -r requirements.txt
streamlit run app.py
